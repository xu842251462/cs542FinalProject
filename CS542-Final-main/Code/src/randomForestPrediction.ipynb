{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "ideal-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make needed imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "naked-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and testing data\n",
    "training_data = pd.read_pickle(\"../data/train.pkl\")\n",
    "testing_data = pd.read_pickle(\"../data/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6   \\\n0     0.866667  0.353333  0.372905  0.041250  0.000000  0.003861  0.639175   \n1     0.533333  0.313333  0.399441  0.050000  0.001156  0.001365  0.639175   \n2     0.400000  0.295556  0.410615  0.052500  0.001156  0.000906  0.639175   \n3     0.466667  0.300000  0.410615  0.051250  0.000578  0.000575  0.628866   \n4     0.333333  0.295556  0.405028  0.053125  0.000578  0.000073  0.659794   \n...        ...       ...       ...       ...       ...       ...       ...   \n1281  0.666667  0.186667  0.734637  0.018125  0.300578  0.002811  0.072165   \n1282  0.866667  0.311111  0.555866  0.015625  0.368786  0.000563  0.958763   \n1283  0.200000  0.011111  0.997207  0.016250  0.234104  0.000000  0.051546   \n1284  0.000000  0.044444  0.927374  0.015000  0.096532  0.000000  0.793814   \n1285  0.933333  0.337778  0.516760  0.014375  0.375145  0.000571  0.917526   \n\n            7         8         9         10        11        12        13  \\\n0     0.003271  0.214381  0.042381  0.614583  0.568899  0.627660  0.000000   \n1     0.012165  0.182423  0.052381  0.635417  0.584296  0.627660  0.000000   \n2     0.015140  0.169108  0.055238  0.572917  0.515012  0.563830  0.000000   \n3     0.008681  0.175766  0.053810  0.625000  0.461124  0.617021  0.000000   \n4     0.009860  0.162450  0.056190  0.572917  0.612779  0.563830  0.000000   \n...        ...       ...       ...       ...       ...       ...       ...   \n1281  0.073767  0.364847  0.020952  0.895833  0.356428  0.893617  0.308911   \n1282  0.000000  0.023968  0.017143  0.958333  0.472671  0.957447  0.132673   \n1283  0.007593  0.328895  0.019524  0.041667  0.444958  0.670213  0.724752   \n1284  0.011283  0.312916  0.018571  0.781250  0.489607  0.776596  0.859406   \n1285  0.000000  0.390146  0.016190  0.927083  0.438799  0.914894  0.122772   \n\n            14        15        16        17        18        19  \n0     0.005254  0.024525  0.649446  0.626335  0.502137  0.347561  \n1     0.003086  0.002110  0.690037  0.669039  0.462607  0.274390  \n2     0.002240  0.002001  0.704797  0.683274  0.432692  0.237805  \n3     0.003056  0.020572  0.697417  0.676157  0.443376  0.256098  \n4     0.000165  0.000199  0.708487  0.690391  0.423077  0.225610  \n...        ...       ...       ...       ...       ...       ...  \n1281  0.005941  0.001149  0.258303  0.245552  0.465812  0.353659  \n1282  0.000886  0.000120  0.225092  0.209964  0.521368  0.378049  \n1283  0.003757  0.009764  0.357934  0.348754  0.446581  0.323171  \n1284  0.004546  0.009446  0.402214  0.395018  0.463675  0.341463  \n1285  0.001022  0.011911  0.214022  0.199288  0.532051  0.371951  \n\n[1286 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.866667</td>\n      <td>0.353333</td>\n      <td>0.372905</td>\n      <td>0.041250</td>\n      <td>0.000000</td>\n      <td>0.003861</td>\n      <td>0.639175</td>\n      <td>0.003271</td>\n      <td>0.214381</td>\n      <td>0.042381</td>\n      <td>0.614583</td>\n      <td>0.568899</td>\n      <td>0.627660</td>\n      <td>0.000000</td>\n      <td>0.005254</td>\n      <td>0.024525</td>\n      <td>0.649446</td>\n      <td>0.626335</td>\n      <td>0.502137</td>\n      <td>0.347561</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.533333</td>\n      <td>0.313333</td>\n      <td>0.399441</td>\n      <td>0.050000</td>\n      <td>0.001156</td>\n      <td>0.001365</td>\n      <td>0.639175</td>\n      <td>0.012165</td>\n      <td>0.182423</td>\n      <td>0.052381</td>\n      <td>0.635417</td>\n      <td>0.584296</td>\n      <td>0.627660</td>\n      <td>0.000000</td>\n      <td>0.003086</td>\n      <td>0.002110</td>\n      <td>0.690037</td>\n      <td>0.669039</td>\n      <td>0.462607</td>\n      <td>0.274390</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.400000</td>\n      <td>0.295556</td>\n      <td>0.410615</td>\n      <td>0.052500</td>\n      <td>0.001156</td>\n      <td>0.000906</td>\n      <td>0.639175</td>\n      <td>0.015140</td>\n      <td>0.169108</td>\n      <td>0.055238</td>\n      <td>0.572917</td>\n      <td>0.515012</td>\n      <td>0.563830</td>\n      <td>0.000000</td>\n      <td>0.002240</td>\n      <td>0.002001</td>\n      <td>0.704797</td>\n      <td>0.683274</td>\n      <td>0.432692</td>\n      <td>0.237805</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.466667</td>\n      <td>0.300000</td>\n      <td>0.410615</td>\n      <td>0.051250</td>\n      <td>0.000578</td>\n      <td>0.000575</td>\n      <td>0.628866</td>\n      <td>0.008681</td>\n      <td>0.175766</td>\n      <td>0.053810</td>\n      <td>0.625000</td>\n      <td>0.461124</td>\n      <td>0.617021</td>\n      <td>0.000000</td>\n      <td>0.003056</td>\n      <td>0.020572</td>\n      <td>0.697417</td>\n      <td>0.676157</td>\n      <td>0.443376</td>\n      <td>0.256098</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.333333</td>\n      <td>0.295556</td>\n      <td>0.405028</td>\n      <td>0.053125</td>\n      <td>0.000578</td>\n      <td>0.000073</td>\n      <td>0.659794</td>\n      <td>0.009860</td>\n      <td>0.162450</td>\n      <td>0.056190</td>\n      <td>0.572917</td>\n      <td>0.612779</td>\n      <td>0.563830</td>\n      <td>0.000000</td>\n      <td>0.000165</td>\n      <td>0.000199</td>\n      <td>0.708487</td>\n      <td>0.690391</td>\n      <td>0.423077</td>\n      <td>0.225610</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1281</th>\n      <td>0.666667</td>\n      <td>0.186667</td>\n      <td>0.734637</td>\n      <td>0.018125</td>\n      <td>0.300578</td>\n      <td>0.002811</td>\n      <td>0.072165</td>\n      <td>0.073767</td>\n      <td>0.364847</td>\n      <td>0.020952</td>\n      <td>0.895833</td>\n      <td>0.356428</td>\n      <td>0.893617</td>\n      <td>0.308911</td>\n      <td>0.005941</td>\n      <td>0.001149</td>\n      <td>0.258303</td>\n      <td>0.245552</td>\n      <td>0.465812</td>\n      <td>0.353659</td>\n    </tr>\n    <tr>\n      <th>1282</th>\n      <td>0.866667</td>\n      <td>0.311111</td>\n      <td>0.555866</td>\n      <td>0.015625</td>\n      <td>0.368786</td>\n      <td>0.000563</td>\n      <td>0.958763</td>\n      <td>0.000000</td>\n      <td>0.023968</td>\n      <td>0.017143</td>\n      <td>0.958333</td>\n      <td>0.472671</td>\n      <td>0.957447</td>\n      <td>0.132673</td>\n      <td>0.000886</td>\n      <td>0.000120</td>\n      <td>0.225092</td>\n      <td>0.209964</td>\n      <td>0.521368</td>\n      <td>0.378049</td>\n    </tr>\n    <tr>\n      <th>1283</th>\n      <td>0.200000</td>\n      <td>0.011111</td>\n      <td>0.997207</td>\n      <td>0.016250</td>\n      <td>0.234104</td>\n      <td>0.000000</td>\n      <td>0.051546</td>\n      <td>0.007593</td>\n      <td>0.328895</td>\n      <td>0.019524</td>\n      <td>0.041667</td>\n      <td>0.444958</td>\n      <td>0.670213</td>\n      <td>0.724752</td>\n      <td>0.003757</td>\n      <td>0.009764</td>\n      <td>0.357934</td>\n      <td>0.348754</td>\n      <td>0.446581</td>\n      <td>0.323171</td>\n    </tr>\n    <tr>\n      <th>1284</th>\n      <td>0.000000</td>\n      <td>0.044444</td>\n      <td>0.927374</td>\n      <td>0.015000</td>\n      <td>0.096532</td>\n      <td>0.000000</td>\n      <td>0.793814</td>\n      <td>0.011283</td>\n      <td>0.312916</td>\n      <td>0.018571</td>\n      <td>0.781250</td>\n      <td>0.489607</td>\n      <td>0.776596</td>\n      <td>0.859406</td>\n      <td>0.004546</td>\n      <td>0.009446</td>\n      <td>0.402214</td>\n      <td>0.395018</td>\n      <td>0.463675</td>\n      <td>0.341463</td>\n    </tr>\n    <tr>\n      <th>1285</th>\n      <td>0.933333</td>\n      <td>0.337778</td>\n      <td>0.516760</td>\n      <td>0.014375</td>\n      <td>0.375145</td>\n      <td>0.000571</td>\n      <td>0.917526</td>\n      <td>0.000000</td>\n      <td>0.390146</td>\n      <td>0.016190</td>\n      <td>0.927083</td>\n      <td>0.438799</td>\n      <td>0.914894</td>\n      <td>0.122772</td>\n      <td>0.001022</td>\n      <td>0.011911</td>\n      <td>0.214022</td>\n      <td>0.199288</td>\n      <td>0.532051</td>\n      <td>0.371951</td>\n    </tr>\n  </tbody>\n</table>\n<p>1286 rows × 20 columns</p>\n</div>"
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "692ae868",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1         2         3         4         5         6   \\\n0    0.000000  0.246085  0.436288  0.058667  0.000000  0.000855  0.628866   \n1    0.166667  0.277405  0.407202  0.058000  0.000000  0.000896  0.649485   \n2    0.083333  0.266219  0.002770  0.058667  0.000000  0.001365  0.639175   \n3    0.916667  0.340045  0.375346  0.046000  0.000000  0.006319  0.670103   \n4    0.000000  0.655481  0.018006  0.000667  0.237402  0.007776  0.969072   \n..        ...       ...       ...       ...       ...       ...       ...   \n357  0.500000  0.185682  0.673130  0.021333  0.115901  0.000877  0.061856   \n358  0.416667  0.306488  0.727147  0.022000  0.125420  0.000150  0.814433   \n359  0.000000  0.022371  0.948753  0.016667  0.095745  0.000000  0.762887   \n360  0.083333  0.011186  0.099723  0.016667  0.247480  0.000000  0.731959   \n361  0.250000  0.000000  1.000000  0.018000  0.243561  0.000000  0.680412   \n\n           7         8       9         10        11        12        13  \\\n0    0.098491  0.151515  0.0610  0.311828  0.504138  0.305263  0.000000   \n1    0.008970  0.163059  0.0610  0.376344  0.580888  0.389474  0.000000   \n2    0.027945  0.157287  0.0610  0.322581  0.501129  0.336842  0.000000   \n3    0.031328  0.223665  0.0465  0.655914  0.558315  0.663158  0.000000   \n4    0.000202  0.633478  0.0005  0.978495  0.368698  0.978947  0.000000   \n..        ...       ...     ...       ...       ...       ...       ...   \n357  0.006014  0.246753  0.0255  0.763441  0.246050  0.042105  0.268924   \n358  0.005160  0.240981  0.0260  0.827957  0.376975  0.810526  0.314741   \n359  0.005946  0.343434  0.0195  0.752688  0.380737  0.747368  0.836653   \n360  0.003417  0.349206  0.0200  0.720430  0.408578  0.705263  0.790837   \n361  0.000348  0.360750  0.0210  0.655914  0.453725  0.642105  0.667331   \n\n           14        15        16        17        18        19  \n0    0.001530  0.002515  0.073801  0.082143  0.369164  0.086420  \n1    0.002602  0.002005  0.723247  0.707143  0.404995  0.123457  \n2    0.002459  0.018632  0.730627  0.075000  0.370250  0.104938  \n3    0.008826  0.003134  0.656827  0.639286  0.502714  0.327160  \n4    0.017504  0.000031  0.073801  0.071429  0.718784  0.376543  \n..        ...       ...       ...       ...       ...       ...  \n357  0.001491  0.010788  0.250923  0.239286  0.534202  0.407407  \n358  0.000152  0.010497  0.254613  0.242857  0.520087  0.395062  \n359  0.007223  0.010483  0.055351  0.057143  0.463626  0.327160  \n360  0.000735  0.000106  0.040590  0.042857  0.463626  0.339506  \n361  0.005979  0.010831  0.343173  0.332143  0.441911  0.290123  \n\n[362 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.246085</td>\n      <td>0.436288</td>\n      <td>0.058667</td>\n      <td>0.000000</td>\n      <td>0.000855</td>\n      <td>0.628866</td>\n      <td>0.098491</td>\n      <td>0.151515</td>\n      <td>0.0610</td>\n      <td>0.311828</td>\n      <td>0.504138</td>\n      <td>0.305263</td>\n      <td>0.000000</td>\n      <td>0.001530</td>\n      <td>0.002515</td>\n      <td>0.073801</td>\n      <td>0.082143</td>\n      <td>0.369164</td>\n      <td>0.086420</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.166667</td>\n      <td>0.277405</td>\n      <td>0.407202</td>\n      <td>0.058000</td>\n      <td>0.000000</td>\n      <td>0.000896</td>\n      <td>0.649485</td>\n      <td>0.008970</td>\n      <td>0.163059</td>\n      <td>0.0610</td>\n      <td>0.376344</td>\n      <td>0.580888</td>\n      <td>0.389474</td>\n      <td>0.000000</td>\n      <td>0.002602</td>\n      <td>0.002005</td>\n      <td>0.723247</td>\n      <td>0.707143</td>\n      <td>0.404995</td>\n      <td>0.123457</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.083333</td>\n      <td>0.266219</td>\n      <td>0.002770</td>\n      <td>0.058667</td>\n      <td>0.000000</td>\n      <td>0.001365</td>\n      <td>0.639175</td>\n      <td>0.027945</td>\n      <td>0.157287</td>\n      <td>0.0610</td>\n      <td>0.322581</td>\n      <td>0.501129</td>\n      <td>0.336842</td>\n      <td>0.000000</td>\n      <td>0.002459</td>\n      <td>0.018632</td>\n      <td>0.730627</td>\n      <td>0.075000</td>\n      <td>0.370250</td>\n      <td>0.104938</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.916667</td>\n      <td>0.340045</td>\n      <td>0.375346</td>\n      <td>0.046000</td>\n      <td>0.000000</td>\n      <td>0.006319</td>\n      <td>0.670103</td>\n      <td>0.031328</td>\n      <td>0.223665</td>\n      <td>0.0465</td>\n      <td>0.655914</td>\n      <td>0.558315</td>\n      <td>0.663158</td>\n      <td>0.000000</td>\n      <td>0.008826</td>\n      <td>0.003134</td>\n      <td>0.656827</td>\n      <td>0.639286</td>\n      <td>0.502714</td>\n      <td>0.327160</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.655481</td>\n      <td>0.018006</td>\n      <td>0.000667</td>\n      <td>0.237402</td>\n      <td>0.007776</td>\n      <td>0.969072</td>\n      <td>0.000202</td>\n      <td>0.633478</td>\n      <td>0.0005</td>\n      <td>0.978495</td>\n      <td>0.368698</td>\n      <td>0.978947</td>\n      <td>0.000000</td>\n      <td>0.017504</td>\n      <td>0.000031</td>\n      <td>0.073801</td>\n      <td>0.071429</td>\n      <td>0.718784</td>\n      <td>0.376543</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>357</th>\n      <td>0.500000</td>\n      <td>0.185682</td>\n      <td>0.673130</td>\n      <td>0.021333</td>\n      <td>0.115901</td>\n      <td>0.000877</td>\n      <td>0.061856</td>\n      <td>0.006014</td>\n      <td>0.246753</td>\n      <td>0.0255</td>\n      <td>0.763441</td>\n      <td>0.246050</td>\n      <td>0.042105</td>\n      <td>0.268924</td>\n      <td>0.001491</td>\n      <td>0.010788</td>\n      <td>0.250923</td>\n      <td>0.239286</td>\n      <td>0.534202</td>\n      <td>0.407407</td>\n    </tr>\n    <tr>\n      <th>358</th>\n      <td>0.416667</td>\n      <td>0.306488</td>\n      <td>0.727147</td>\n      <td>0.022000</td>\n      <td>0.125420</td>\n      <td>0.000150</td>\n      <td>0.814433</td>\n      <td>0.005160</td>\n      <td>0.240981</td>\n      <td>0.0260</td>\n      <td>0.827957</td>\n      <td>0.376975</td>\n      <td>0.810526</td>\n      <td>0.314741</td>\n      <td>0.000152</td>\n      <td>0.010497</td>\n      <td>0.254613</td>\n      <td>0.242857</td>\n      <td>0.520087</td>\n      <td>0.395062</td>\n    </tr>\n    <tr>\n      <th>359</th>\n      <td>0.000000</td>\n      <td>0.022371</td>\n      <td>0.948753</td>\n      <td>0.016667</td>\n      <td>0.095745</td>\n      <td>0.000000</td>\n      <td>0.762887</td>\n      <td>0.005946</td>\n      <td>0.343434</td>\n      <td>0.0195</td>\n      <td>0.752688</td>\n      <td>0.380737</td>\n      <td>0.747368</td>\n      <td>0.836653</td>\n      <td>0.007223</td>\n      <td>0.010483</td>\n      <td>0.055351</td>\n      <td>0.057143</td>\n      <td>0.463626</td>\n      <td>0.327160</td>\n    </tr>\n    <tr>\n      <th>360</th>\n      <td>0.083333</td>\n      <td>0.011186</td>\n      <td>0.099723</td>\n      <td>0.016667</td>\n      <td>0.247480</td>\n      <td>0.000000</td>\n      <td>0.731959</td>\n      <td>0.003417</td>\n      <td>0.349206</td>\n      <td>0.0200</td>\n      <td>0.720430</td>\n      <td>0.408578</td>\n      <td>0.705263</td>\n      <td>0.790837</td>\n      <td>0.000735</td>\n      <td>0.000106</td>\n      <td>0.040590</td>\n      <td>0.042857</td>\n      <td>0.463626</td>\n      <td>0.339506</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>0.250000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.018000</td>\n      <td>0.243561</td>\n      <td>0.000000</td>\n      <td>0.680412</td>\n      <td>0.000348</td>\n      <td>0.360750</td>\n      <td>0.0210</td>\n      <td>0.655914</td>\n      <td>0.453725</td>\n      <td>0.642105</td>\n      <td>0.667331</td>\n      <td>0.005979</td>\n      <td>0.010831</td>\n      <td>0.343173</td>\n      <td>0.332143</td>\n      <td>0.441911</td>\n      <td>0.290123</td>\n    </tr>\n  </tbody>\n</table>\n<p>362 rows × 20 columns</p>\n</div>"
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "important-grove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.35333333, 0.31333333, 0.29555556, ..., 0.01111111, 0.04444444,\n       0.33777778])"
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate training X and Y values\n",
    "train_X = training_data.iloc[:, 2:20].values\n",
    "train_Y = training_data.iloc[:, 1].values\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "24b2aa58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.37290503, 0.04125   , 0.        , ..., 0.62633452, 0.50213675,\n        0.34756098],\n       [0.39944134, 0.05      , 0.00115607, ..., 0.66903915, 0.46260684,\n        0.27439024],\n       [0.41061453, 0.0525    , 0.00115607, ..., 0.68327402, 0.43269231,\n        0.23780488],\n       ...,\n       [0.9972067 , 0.01625   , 0.23410405, ..., 0.34875445, 0.4465812 ,\n        0.32317073],\n       [0.9273743 , 0.015     , 0.09653179, ..., 0.39501779, 0.46367521,\n        0.34146341],\n       [0.51675978, 0.014375  , 0.37514451, ..., 0.19928826, 0.53205128,\n        0.37195122]])"
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do not include the year column\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.24608501, 0.27740492, 0.26621924, 0.34004474, 0.65548098,\n       0.63758389, 0.64876957, 0.72930649, 0.63982103, 0.65100671,\n       0.65995526, 0.11856823, 0.10738255, 0.66666667, 0.68008949,\n       0.68456376, 0.63310962, 0.63310962, 0.64205817, 0.79642058,\n       0.93288591, 0.82102908, 0.76733781, 0.76957494, 0.78299776,\n       0.52572707, 0.53914989, 0.53914989, 0.51454139, 0.52572707,\n       0.53467562, 0.52348993, 0.51230425, 0.53467562, 0.75391499,\n       0.75391499, 0.77181208, 0.53467562, 0.54138702, 0.5458613 ,\n       0.25727069, 0.2639821 , 0.27293065, 0.38926174, 0.40715884,\n       0.44519016, 0.68680089, 0.70246085, 0.69574944, 0.05369128,\n       0.03803132, 0.08501119, 0.59731544, 0.60626398, 0.6196868 ,\n       0.61073826, 0.61521253, 0.62416107, 0.29530201, 0.2639821 ,\n       0.20357942, 0.21923937, 0.23489933, 0.59955257, 0.60626398,\n       0.61521253, 0.49888143, 0.47651007, 0.20805369, 0.20134228,\n       0.79865772, 0.82102908, 0.91051454, 0.10961969, 0.19463087,\n       0.16778523, 0.28411633, 0.77852349, 0.78970917, 0.62416107,\n       0.63534676, 0.65324385, 0.60850112, 0.61521253, 0.63758389,\n       0.34675615, 0.35123043, 0.41834452, 0.74272931, 0.7606264 ,\n       0.74720358, 0.72259508, 0.71588367, 0.75838926, 0.76286353,\n       0.76733781, 0.3803132 , 0.55257271, 0.60178971, 0.60626398,\n       0.55928412, 0.65100671, 0.65548098, 0.67337808, 0.55033557,\n       0.64205817, 0.57494407, 0.3310962 , 0.32438479, 0.63758389,\n       0.64205817, 0.64205817, 0.39149888, 0.53020134, 0.52572707,\n       0.52796421, 0.53243848, 0.77628635, 0.78076063, 0.84340045,\n       0.38702461, 0.38255034, 0.64205817, 0.61297539, 0.62639821,\n       0.7606264 , 0.76286353, 0.77852349, 0.30425056, 0.31319911,\n       0.32662192, 0.76957494, 0.77628635, 0.78076063, 0.68680089,\n       0.56823266, 0.30201342, 0.29082774, 0.27740492, 0.29753915,\n       0.46979866, 0.46532438, 0.46308725, 0.41163311, 0.60402685,\n       0.61073826, 0.62416107, 0.44966443, 0.45861298, 0.46756152,\n       0.4966443 , 0.50111857, 0.46979866, 0.5033557 , 0.45637584,\n       0.48322148, 0.88814318, 0.78299776, 0.78299776, 0.82102908,\n       0.79418345, 0.79865772, 1.        , 0.64876957, 0.65324385,\n       0.66442953, 0.61744966, 0.62192394, 0.63087248, 0.44966443,\n       0.45637584, 0.45637584, 0.18120805, 0.19463087, 0.21923937,\n       0.44742729, 0.4541387 , 0.45861298, 0.57270694, 0.64205817,\n       0.59731544, 0.64205817, 0.64653244, 0.65771812, 0.01118568,\n       0.00447427, 0.02237136, 0.50782998, 0.33333333, 0.60178971,\n       0.60626398, 0.61073826, 0.75391499, 0.7606264 , 0.76957494,\n       0.34899329, 0.44071588, 0.55257271, 0.00671141, 0.01789709,\n       0.03803132, 0.63534676, 0.63982103, 0.64653244, 0.75391499,\n       0.61521253, 0.65100671, 0.1901566 , 0.20805369, 0.22371365,\n       0.76957494, 0.77628635, 0.78299776, 0.37807606, 0.55257271,\n       0.60850112, 0.60850112, 0.61744966, 0.68680089, 0.68680089,\n       0.69574944, 0.42281879, 0.43624161, 0.44071588, 0.6935123 ,\n       0.67785235, 0.55257271, 0.56375839, 0.6196868 , 0.12304251,\n       0.21700224, 0.3064877 , 0.4295302 , 0.43847875, 0.44519016,\n       0.44742729, 0.45637584, 0.47203579, 0.48545861, 0.82326622,\n       0.64205817, 0.68680089, 0.59731544, 0.31096197, 0.41834452,\n       0.2393736 , 0.12304251, 0.43400447, 0.41610738, 0.44519016,\n       0.70246085, 0.70469799, 0.70469799, 0.3310962 , 0.33557047,\n       0.34899329, 0.61744966, 0.62192394, 0.62639821, 0.62416107,\n       0.63758389, 0.6689038 , 0.5033557 , 0.5033557 , 0.51454139,\n       0.6689038 , 0.67561521, 0.68456376, 0.72930649, 0.7360179 ,\n       0.75391499, 0.75391499, 0.73154362, 0.61297539, 0.45861298,\n       0.46085011, 0.46308725, 0.17225951, 0.20357942, 0.24608501,\n       0.68680089, 0.70917226, 0.60626398, 0.43624161, 0.44742729,\n       0.45637584, 0.46308725, 0.3803132 , 0.39821029, 0.64205817,\n       0.65548098, 0.65995526, 0.6196868 , 0.62192394, 0.62192394,\n       0.08501119, 0.06263982, 0.48993289, 0.4966443 , 0.51006711,\n       0.26174497, 0.23713647, 0.21029083, 0.7852349 , 0.78747204,\n       0.82102908, 0.55480984, 0.6689038 , 0.65995526, 0.56375839,\n       0.55928412, 0.06263982, 0.04697987, 0.02908277, 0.83668904,\n       0.64205817, 0.63758389, 0.46756152, 0.48322148, 0.47427293,\n       0.60178971, 0.60626398, 0.61073826, 0.51677852, 0.49888143,\n       0.32662192, 0.27740492, 0.61521253, 0.61744966, 0.62416107,\n       0.55928412, 0.56152125, 0.56375839, 0.64653244, 0.65324385,\n       0.66442953, 0.75391499, 0.60178971, 0.6196868 , 0.42729306,\n       0.4295302 , 0.42505593, 0.14988814, 0.15659955, 0.19910515,\n       0.52348993, 0.5212528 , 0.51677852, 0.69127517, 0.69574944,\n       0.69574944, 0.51230425, 0.52572707, 0.51454139, 0.55480984,\n       0.55928412, 0.56599553, 0.18568233, 0.3064877 , 0.02237136,\n       0.01118568, 0.        ])"
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load testing values\n",
    "test_X = testing_data.iloc[:, 2:20].values\n",
    "test_Y = testing_data.iloc[:, 1].values\n",
    "test_Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.43628809, 0.05866667, 0.        , ..., 0.08214286, 0.36916395,\n        0.08641975],\n       [0.40720222, 0.058     , 0.        , ..., 0.70714286, 0.40499457,\n        0.12345679],\n       [0.00277008, 0.05866667, 0.        , ..., 0.075     , 0.37024973,\n        0.10493827],\n       ...,\n       [0.94875346, 0.01666667, 0.09574468, ..., 0.05714286, 0.46362649,\n        0.32716049],\n       [0.09972299, 0.01666667, 0.2474804 , ..., 0.04285714, 0.46362649,\n        0.33950617],\n       [1.        , 0.018     , 0.24356103, ..., 0.33214286, 0.44191097,\n        0.29012346]])"
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average baseline error:  0.23\n"
     ]
    }
   ],
   "source": [
    "#basline estimate\n",
    "#getting life_expectancy from the oroginal testing data\n",
    "baseline_preds = testing_data.iloc[:, 1:2].values\n",
    "# print(baseline_preds)\n",
    "# Baseline errors, and display average baseline error\n",
    "base_errors = abs(baseline_preds - test_Y)\n",
    "print('Average baseline error: ', round(np.mean(base_errors), 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "cef65bf4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27460651 0.28784941 0.50808334 0.34850011 0.65356395 0.64879641\n",
      " 0.65612807 0.72154081 0.62199761 0.629066   0.64567258 0.19757305\n",
      " 0.19983015 0.68109976 0.72739936 0.70010417 0.63873718 0.64426536\n",
      " 0.64673479 0.88036483 0.85528564 0.84164805 0.78525851 0.81780272\n",
      " 0.81017157 0.61530271 0.62027399 0.61478706 0.50442527 0.53588944\n",
      " 0.54318453 0.54330995 0.54479089 0.55502798 0.83574119 0.80861877\n",
      " 0.84624491 0.56079393 0.56694456 0.57106647 0.27236838 0.27127906\n",
      " 0.28315026 0.52278154 0.39782054 0.42462783 0.62815416 0.65360627\n",
      " 0.66518339 0.13984777 0.11604935 0.17861474 0.63614708 0.64796197\n",
      " 0.65688726 0.65105379 0.63884495 0.64334551 0.32003904 0.29672478\n",
      " 0.24667924 0.25090102 0.23165661 0.60933516 0.61641067 0.61072097\n",
      " 0.4977156  0.46318433 0.2471886  0.24275218 0.84167686 0.87120651\n",
      " 0.87866624 0.12751067 0.17402389 0.17579836 0.21862878 0.79610251\n",
      " 0.80048643 0.62466227 0.62679192 0.63304777 0.62860463 0.63523925\n",
      " 0.64781178 0.39354958 0.38978377 0.40083693 0.68553319 0.70018938\n",
      " 0.73201348 0.74972778 0.75513615 0.77524905 0.77790121 0.80969971\n",
      " 0.39403242 0.39446003 0.59382742 0.58810646 0.56159411 0.63969502\n",
      " 0.64860153 0.66748398 0.56142501 0.59417103 0.59356204 0.32012352\n",
      " 0.31242622 0.67952086 0.6813734  0.68509213 0.32690751 0.31912236\n",
      " 0.54066777 0.55622664 0.55679426 0.84363178 0.8588401  0.89073006\n",
      " 0.39544567 0.39390687 0.64522646 0.6407186  0.66476272 0.85327525\n",
      " 0.8509816  0.83272826 0.27398099 0.30761201 0.3432322  0.79141409\n",
      " 0.81358849 0.81892171 0.64214763 0.61611226 0.29244842 0.24730921\n",
      " 0.28337685 0.29353555 0.48130071 0.48524882 0.48476851 0.4164025\n",
      " 0.58714575 0.61069314 0.62902013 0.46525753 0.50940651 0.5068365\n",
      " 0.51424955 0.51745697 0.51814297 0.56336441 0.50703429 0.5266497\n",
      " 0.85699685 0.86096314 0.85771868 0.84211761 0.82853321 0.85098528\n",
      " 0.84902169 0.64335819 0.65742155 0.67293467 0.64347087 0.6453394\n",
      " 0.64788793 0.4712423  0.49074932 0.49395247 0.15669975 0.16282716\n",
      " 0.21371686 0.48181109 0.48753894 0.48847025 0.57789013 0.58452071\n",
      " 0.67133573 0.63585625 0.62381098 0.64059211 0.05033488 0.08028145\n",
      " 0.0814589  0.35565816 0.33917802 0.63207723 0.63092688 0.62927185\n",
      " 0.80140578 0.80644703 0.84146101 0.34943827 0.37090603 0.40384638\n",
      " 0.07268189 0.08606332 0.10756519 0.6442443  0.64795345 0.66044406\n",
      " 0.62942729 0.63894707 0.65086619 0.26239585 0.26786358 0.26836569\n",
      " 0.79403895 0.86346438 0.85581195 0.40643389 0.4160621  0.61691105\n",
      " 0.59986381 0.63084444 0.68950343 0.68980046 0.70097614 0.45820879\n",
      " 0.46603669 0.45759786 0.70150687 0.68335042 0.5590148  0.57787067\n",
      " 0.63360976 0.17091558 0.17539298 0.16317428 0.45627971 0.44579419\n",
      " 0.46172736 0.43700272 0.47429759 0.49429605 0.53026877 0.84356372\n",
      " 0.59623103 0.60842489 0.59133784 0.32457559 0.35130849 0.22544558\n",
      " 0.17165605 0.47360265 0.49078725 0.47899104 0.68401494 0.6840375\n",
      " 0.69883545 0.37066257 0.37886167 0.39440987 0.63457904 0.63967269\n",
      " 0.64144642 0.66436639 0.66913784 0.67457118 0.51468093 0.51650942\n",
      " 0.52672466 0.67393633 0.68430834 0.67553532 0.74710085 0.75638653\n",
      " 0.76905655 0.6195693  0.62331105 0.62840071 0.51480205 0.51661955\n",
      " 0.51926478 0.25011693 0.25797042 0.26136925 0.66050434 0.66816211\n",
      " 0.63540207 0.43827486 0.42873634 0.46366252 0.39249908 0.40274262\n",
      " 0.41315138 0.66943568 0.66666131 0.66657467 0.60796963 0.61744879\n",
      " 0.6122983  0.18692248 0.18093751 0.50306647 0.51928153 0.53613872\n",
      " 0.27906988 0.2673959  0.2450827  0.81991996 0.84805022 0.87164092\n",
      " 0.58031106 0.66542417 0.66302017 0.5944702  0.56333934 0.08683801\n",
      " 0.08527988 0.09101939 0.86215067 0.63949702 0.63098853 0.5139869\n",
      " 0.52129173 0.50812731 0.57174264 0.57715589 0.59045242 0.53567716\n",
      " 0.51994226 0.34925631 0.32379111 0.63741823 0.63950405 0.63821243\n",
      " 0.57162009 0.60119983 0.59602632 0.65238525 0.62998021 0.6603951\n",
      " 0.63191155 0.65193226 0.64358158 0.46355929 0.46294004 0.4401293\n",
      " 0.22591941 0.20886921 0.22658665 0.53608323 0.53773444 0.54143239\n",
      " 0.70864857 0.70607124 0.71955738 0.53340726 0.5390087  0.5301507\n",
      " 0.5443507  0.5588114  0.56982766 0.17086533 0.15998844 0.0785354\n",
      " 0.11031006 0.03122944]\n"
     ]
    }
   ],
   "source": [
    "#Training Model using RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 300, max_leaf_nodes = 300, bootstrap = True)\n",
    "# Fit the regressor with x and y data\n",
    "regressor.fit(train_X, train_Y)\n",
    "# Predict with testing X\n",
    "predict_Y = regressor.predict(test_X)\n",
    "print(predict_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "8954ac93",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.03 degrees.\n"
     ]
    }
   ],
   "source": [
    "# making prediction on the test set, we will get the prediction life expectancy\n",
    "predict_Y = regressor.predict(test_X)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predict_Y - test_Y)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "ba1c2a22",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.78 %.\n"
     ]
    }
   ],
   "source": [
    "#Determine Performance Metrics\n",
    "# Calculate mean absolute percentage error\n",
    "percentage_error = 100 * (errors / predict_Y)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(percentage_error)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "eced860a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:                   17 Importance: 0.55\n",
      "Variable:                   12 Importance: 0.25\n",
      "Variable:                    1 Importance: 0.11\n",
      "Variable:                    3 Importance: 0.01\n",
      "Variable:                    7 Importance: 0.01\n",
      "Variable:                   10 Importance: 0.01\n",
      "Variable:                   15 Importance: 0.01\n",
      "Variable:                   16 Importance: 0.01\n",
      "Variable:                   18 Importance: 0.01\n",
      "Variable:                    2 Importance: 0.0\n",
      "Variable:                    4 Importance: 0.0\n",
      "Variable:                    5 Importance: 0.0\n",
      "Variable:                    6 Importance: 0.0\n",
      "Variable:                    8 Importance: 0.0\n",
      "Variable:                    9 Importance: 0.0\n",
      "Variable:                   11 Importance: 0.0\n",
      "Variable:                   13 Importance: 0.0\n",
      "Variable:                   14 Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "#figuring out the usefulness of all the variable in the entire random forest\n",
    "# Get numerical feature importances\n",
    "testing_data_without_year = testing_data.drop(testing_data.columns[[0]],axis = 1)\n",
    "# print(testing_data_without_year)\n",
    "importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(testing_data_without_year, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "0356118d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#picking up the most important variables to recalculate the importance\n",
    "updated_testing = testing_data.iloc[:, [1,3,7,10,15,16,18,12,17]]\n",
    "test_important = testing_data.iloc[:, [1,3,7,10,15,16,18,12,17]].values\n",
    "train_important = training_data.iloc[:, [1,3,7,10,15,16,18,12,17]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "3cd56408",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:                    1 Importance: 0.11\n",
      "Variable:                    7 Importance: 0.01\n",
      "Variable:                   18 Importance: 0.01\n",
      "Variable:                    3 Importance: 0.0\n",
      "Variable:                   10 Importance: 0.0\n",
      "Variable:                   15 Importance: 0.0\n",
      "Variable:                   16 Importance: 0.0\n",
      "Variable:                   12 Importance: 0.0\n",
      "Variable:                   17 Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "updated_importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(updated_testing, updated_importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "9e27e732",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.0009773725113743953 degrees.\n",
      "Accuracy: -inf %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xu842\\AppData\\Local\\Temp\\ipykernel_54568\\2633863536.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  updated_error_percentage = np.mean(100 * (errors / test_Y))\n"
     ]
    }
   ],
   "source": [
    "# New random forest with only the two most important variables\n",
    "rf_most_important = RandomForestRegressor(n_estimators = 300, max_leaf_nodes = 300, bootstrap = True)\n",
    "# Train the random forest\n",
    "rf_most_important.fit(train_important, train_Y)\n",
    "# Make predictions and determine the error\n",
    "predictions = rf_most_important.predict(test_important)\n",
    "errors = abs(predictions - test_Y)\n",
    "# Display the performance metrics\n",
    "print('Mean Absolute Error:', np.mean(errors), 'degrees.')\n",
    "updated_error_percentage = np.mean(100 * (errors / test_Y))\n",
    "accuracy = 100 - updated_error_percentage\n",
    "print('Accuracy:', accuracy, '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "a985c98a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (18), usually from a call to set_ticks, does not match the number of ticklabels (20).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [537]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(x_value)\n\u001B[0;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mbar(x_value, importances, orientation \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mxticks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtesting_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrotation\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportance\u001B[39m\u001B[38;5;124m'\u001B[39m);\n\u001B[0;32m     11\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlable(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVariable\u001B[39m\u001B[38;5;124m'\u001B[39m);\n",
      "File \u001B[1;32mc:\\users\\xu842\\pycharmprojects\\leetcode2021python\\venv\\lib\\site-packages\\matplotlib\\pyplot.py:1816\u001B[0m, in \u001B[0;36mxticks\u001B[1;34m(ticks, labels, **kwargs)\u001B[0m\n\u001B[0;32m   1814\u001B[0m         l\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n\u001B[0;32m   1815\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1816\u001B[0m     labels \u001B[38;5;241m=\u001B[39m ax\u001B[38;5;241m.\u001B[39mset_xticklabels(labels, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1818\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m locs, labels\n",
      "File \u001B[1;32mc:\\users\\xu842\\pycharmprojects\\leetcode2021python\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py:75\u001B[0m, in \u001B[0;36m_axis_method_wrapper.__set_name__.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m get_method(\u001B[38;5;28mself\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\xu842\\pycharmprojects\\leetcode2021python\\venv\\lib\\site-packages\\matplotlib\\axis.py:1798\u001B[0m, in \u001B[0;36mAxis._set_ticklabels\u001B[1;34m(self, labels, fontdict, minor, **kwargs)\u001B[0m\n\u001B[0;32m   1796\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fontdict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1797\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mupdate(fontdict)\n\u001B[1;32m-> 1798\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_ticklabels(labels, minor\u001B[38;5;241m=\u001B[39mminor, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\xu842\\pycharmprojects\\leetcode2021python\\venv\\lib\\site-packages\\matplotlib\\axis.py:1720\u001B[0m, in \u001B[0;36mAxis.set_ticklabels\u001B[1;34m(self, ticklabels, minor, **kwargs)\u001B[0m\n\u001B[0;32m   1716\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(locator, mticker\u001B[38;5;241m.\u001B[39mFixedLocator):\n\u001B[0;32m   1717\u001B[0m     \u001B[38;5;66;03m# Passing [] as a list of ticklabels is often used as a way to\u001B[39;00m\n\u001B[0;32m   1718\u001B[0m     \u001B[38;5;66;03m# remove all tick labels, so only error for > 0 ticklabels\u001B[39;00m\n\u001B[0;32m   1719\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(locator\u001B[38;5;241m.\u001B[39mlocs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(ticklabels) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(ticklabels) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 1720\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1721\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe number of FixedLocator locations\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1722\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(locator\u001B[38;5;241m.\u001B[39mlocs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m), usually from a call to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1723\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m set_ticks, does not match\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1724\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m the number of ticklabels (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(ticklabels)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1725\u001B[0m     tickd \u001B[38;5;241m=\u001B[39m {loc: lab \u001B[38;5;28;01mfor\u001B[39;00m loc, lab \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(locator\u001B[38;5;241m.\u001B[39mlocs, ticklabels)}\n\u001B[0;32m   1726\u001B[0m     func \u001B[38;5;241m=\u001B[39m functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_with_dict, tickd)\n",
      "\u001B[1;31mValueError\u001B[0m: The number of FixedLocator locations (18), usually from a call to set_ticks, does not match the number of ticklabels (20)."
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_value = list(range(len(importances)))\n",
    "print(x_value)\n",
    "\n",
    "plt.bar(x_value, importances, orientation = 'vertical')\n",
    "plt.xticks(x_value, testing_data, rotation = 6)\n",
    "\n",
    "plt.ylabel('importance');\n",
    "plt.xlable('Variable');\n",
    "plt.title('Variable Importance');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "meaningful-reading",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss on testing set: 0.0022976552878079277\n",
      "Random forest regressor score: 0.9453089025109975\n"
     ]
    }
   ],
   "source": [
    "errors = mean_squared_error(test_Y, predict_Y)\n",
    "print(\"MSE loss on testing set: \" + str(mean_squared_error(test_Y, predict_Y)))\n",
    "print(\"Random forest regressor score: \" + str(regressor.score(test_X, test_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f84d1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}