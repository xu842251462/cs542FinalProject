{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "ideal-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make needed imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "naked-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and testing data\n",
    "training_data = pd.read_pickle(\"../data/train.pkl\")\n",
    "testing_data = pd.read_pickle(\"../data/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2222a2e3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6   \\\n0     0.353333  0.373961  0.040000  0.000000  0.003878  0.618557  0.003743   \n1     0.353333  0.369806  0.041250  0.000000  0.003861  0.639175  0.003271   \n2     0.337778  0.379501  0.044375  0.000000  0.000374  0.680412  0.022923   \n3     0.328889  0.385042  0.046250  0.000000  0.004202  0.659794  0.015132   \n4     0.324444  0.387812  0.048125  0.000000  0.002994  0.628866  0.021766   \n...        ...       ...       ...       ...       ...       ...       ...   \n1300  0.006667  1.000000  0.016875  0.251445  0.000000  0.680412  0.000236   \n1301  0.011111  0.988920  0.016250  0.234104  0.000000  0.051546  0.007593   \n1302  0.017778  0.099723  0.015625  0.255491  0.000000  0.731959  0.002313   \n1303  0.028889  0.948753  0.015625  0.098844  0.000000  0.762887  0.004025   \n1304  0.044444  0.919668  0.015000  0.096532  0.000000  0.793814  0.011283   \n\n            7         8         9         10        11        12        13  \\\n0     0.220000  0.040952  0.568421  0.544387  0.610526  0.000000  0.005346   \n1     0.213333  0.042381  0.610526  0.540719  0.631579  0.000000  0.005513   \n2     0.201333  0.046190  0.673684  0.521643  0.673684  0.000000  0.000541   \n3     0.194667  0.048571  0.652632  0.619222  0.652632  0.000000  0.004827   \n4     0.188000  0.050476  0.621053  0.635363  0.621053  0.000000  0.003887   \n...        ...       ...       ...       ...       ...       ...       ...   \n1300  0.333333  0.020000  0.663158  0.467351  0.642105  0.663366  0.003961   \n1301  0.328000  0.019524  0.031579  0.422597  0.673684  0.724752  0.003952   \n1302  0.322667  0.019048  0.726316  0.423331  0.705263  0.786139  0.000487   \n1303  0.317333  0.018571  0.757895  0.396185  0.747368  0.831683  0.004785   \n1304  0.312000  0.018571  0.778947  0.465150  0.778947  0.859406  0.004774   \n\n            14        15        16        17        18        19  \n0     0.000253  0.642066  0.619217  0.508547  0.351515  0.768400  \n1     0.024525  0.649446  0.626335  0.502137  0.345455  0.592494  \n2     0.002302  0.667897  0.644128  0.485043  0.321212  0.630311  \n3     0.002228  0.675277  0.651246  0.478632  0.303030  0.418208  \n4     0.000220  0.682657  0.661922  0.463675  0.284848  0.820511  \n...        ...       ...       ...       ...       ...       ...  \n1300  0.009875  0.343173  0.330961  0.434829  0.303030  0.588091  \n1301  0.009764  0.357934  0.348754  0.446581  0.321212  0.510390  \n1302  0.000097  0.040590  0.042705  0.456197  0.351515  0.456825  \n1303  0.009558  0.055351  0.056940  0.456197  0.339394  0.831598  \n1304  0.009446  0.402214  0.395018  0.463675  0.339394  0.318853  \n\n[1305 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.353333</td>\n      <td>0.373961</td>\n      <td>0.040000</td>\n      <td>0.000000</td>\n      <td>0.003878</td>\n      <td>0.618557</td>\n      <td>0.003743</td>\n      <td>0.220000</td>\n      <td>0.040952</td>\n      <td>0.568421</td>\n      <td>0.544387</td>\n      <td>0.610526</td>\n      <td>0.000000</td>\n      <td>0.005346</td>\n      <td>0.000253</td>\n      <td>0.642066</td>\n      <td>0.619217</td>\n      <td>0.508547</td>\n      <td>0.351515</td>\n      <td>0.768400</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.353333</td>\n      <td>0.369806</td>\n      <td>0.041250</td>\n      <td>0.000000</td>\n      <td>0.003861</td>\n      <td>0.639175</td>\n      <td>0.003271</td>\n      <td>0.213333</td>\n      <td>0.042381</td>\n      <td>0.610526</td>\n      <td>0.540719</td>\n      <td>0.631579</td>\n      <td>0.000000</td>\n      <td>0.005513</td>\n      <td>0.024525</td>\n      <td>0.649446</td>\n      <td>0.626335</td>\n      <td>0.502137</td>\n      <td>0.345455</td>\n      <td>0.592494</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.337778</td>\n      <td>0.379501</td>\n      <td>0.044375</td>\n      <td>0.000000</td>\n      <td>0.000374</td>\n      <td>0.680412</td>\n      <td>0.022923</td>\n      <td>0.201333</td>\n      <td>0.046190</td>\n      <td>0.673684</td>\n      <td>0.521643</td>\n      <td>0.673684</td>\n      <td>0.000000</td>\n      <td>0.000541</td>\n      <td>0.002302</td>\n      <td>0.667897</td>\n      <td>0.644128</td>\n      <td>0.485043</td>\n      <td>0.321212</td>\n      <td>0.630311</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.328889</td>\n      <td>0.385042</td>\n      <td>0.046250</td>\n      <td>0.000000</td>\n      <td>0.004202</td>\n      <td>0.659794</td>\n      <td>0.015132</td>\n      <td>0.194667</td>\n      <td>0.048571</td>\n      <td>0.652632</td>\n      <td>0.619222</td>\n      <td>0.652632</td>\n      <td>0.000000</td>\n      <td>0.004827</td>\n      <td>0.002228</td>\n      <td>0.675277</td>\n      <td>0.651246</td>\n      <td>0.478632</td>\n      <td>0.303030</td>\n      <td>0.418208</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.324444</td>\n      <td>0.387812</td>\n      <td>0.048125</td>\n      <td>0.000000</td>\n      <td>0.002994</td>\n      <td>0.628866</td>\n      <td>0.021766</td>\n      <td>0.188000</td>\n      <td>0.050476</td>\n      <td>0.621053</td>\n      <td>0.635363</td>\n      <td>0.621053</td>\n      <td>0.000000</td>\n      <td>0.003887</td>\n      <td>0.000220</td>\n      <td>0.682657</td>\n      <td>0.661922</td>\n      <td>0.463675</td>\n      <td>0.284848</td>\n      <td>0.820511</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1300</th>\n      <td>0.006667</td>\n      <td>1.000000</td>\n      <td>0.016875</td>\n      <td>0.251445</td>\n      <td>0.000000</td>\n      <td>0.680412</td>\n      <td>0.000236</td>\n      <td>0.333333</td>\n      <td>0.020000</td>\n      <td>0.663158</td>\n      <td>0.467351</td>\n      <td>0.642105</td>\n      <td>0.663366</td>\n      <td>0.003961</td>\n      <td>0.009875</td>\n      <td>0.343173</td>\n      <td>0.330961</td>\n      <td>0.434829</td>\n      <td>0.303030</td>\n      <td>0.588091</td>\n    </tr>\n    <tr>\n      <th>1301</th>\n      <td>0.011111</td>\n      <td>0.988920</td>\n      <td>0.016250</td>\n      <td>0.234104</td>\n      <td>0.000000</td>\n      <td>0.051546</td>\n      <td>0.007593</td>\n      <td>0.328000</td>\n      <td>0.019524</td>\n      <td>0.031579</td>\n      <td>0.422597</td>\n      <td>0.673684</td>\n      <td>0.724752</td>\n      <td>0.003952</td>\n      <td>0.009764</td>\n      <td>0.357934</td>\n      <td>0.348754</td>\n      <td>0.446581</td>\n      <td>0.321212</td>\n      <td>0.510390</td>\n    </tr>\n    <tr>\n      <th>1302</th>\n      <td>0.017778</td>\n      <td>0.099723</td>\n      <td>0.015625</td>\n      <td>0.255491</td>\n      <td>0.000000</td>\n      <td>0.731959</td>\n      <td>0.002313</td>\n      <td>0.322667</td>\n      <td>0.019048</td>\n      <td>0.726316</td>\n      <td>0.423331</td>\n      <td>0.705263</td>\n      <td>0.786139</td>\n      <td>0.000487</td>\n      <td>0.000097</td>\n      <td>0.040590</td>\n      <td>0.042705</td>\n      <td>0.456197</td>\n      <td>0.351515</td>\n      <td>0.456825</td>\n    </tr>\n    <tr>\n      <th>1303</th>\n      <td>0.028889</td>\n      <td>0.948753</td>\n      <td>0.015625</td>\n      <td>0.098844</td>\n      <td>0.000000</td>\n      <td>0.762887</td>\n      <td>0.004025</td>\n      <td>0.317333</td>\n      <td>0.018571</td>\n      <td>0.757895</td>\n      <td>0.396185</td>\n      <td>0.747368</td>\n      <td>0.831683</td>\n      <td>0.004785</td>\n      <td>0.009558</td>\n      <td>0.055351</td>\n      <td>0.056940</td>\n      <td>0.456197</td>\n      <td>0.339394</td>\n      <td>0.831598</td>\n    </tr>\n    <tr>\n      <th>1304</th>\n      <td>0.044444</td>\n      <td>0.919668</td>\n      <td>0.015000</td>\n      <td>0.096532</td>\n      <td>0.000000</td>\n      <td>0.793814</td>\n      <td>0.011283</td>\n      <td>0.312000</td>\n      <td>0.018571</td>\n      <td>0.778947</td>\n      <td>0.465150</td>\n      <td>0.778947</td>\n      <td>0.859406</td>\n      <td>0.004774</td>\n      <td>0.009446</td>\n      <td>0.402214</td>\n      <td>0.395018</td>\n      <td>0.463675</td>\n      <td>0.339394</td>\n      <td>0.318853</td>\n    </tr>\n  </tbody>\n</table>\n<p>1305 rows × 20 columns</p>\n</div>"
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "692ae868",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1         2         3         4         5         6   \\\n0    0.441860  0.375358  0.044286  0.000000  0.004170  0.649485  0.009290   \n1    0.313953  0.388252  0.049286  0.000000  0.004575  0.670103  0.022436   \n2    0.281395  0.409742  0.057143  0.001120  0.001503  0.639175  0.012872   \n3    0.267442  0.421203  0.058571  0.000560  0.000625  0.628866  0.009185   \n4    0.262791  0.421203  0.060000  0.001120  0.000992  0.639175  0.016020   \n..        ...       ...       ...       ...       ...       ...       ...   \n339  0.306977  0.498567  0.020714  0.144457  0.011548  0.783505  0.007213   \n340  0.283721  0.522923  0.020714  0.143337  0.010733  0.814433  0.106538   \n341  0.279070  0.752149  0.023571  0.125420  0.000093  0.814433  0.003695   \n342  0.093023  0.839542  0.021429  0.259239  0.000045  0.731959  0.006867   \n343  0.051163  0.904011  0.021429  0.198768  0.001208  0.752577  0.000000   \n\n           7         8         9         10        11        12        13  \\\n0    0.232022  0.043684  0.031250  0.666667  0.649485  0.000000  0.004855   \n1    0.211669  0.048947  0.666667  0.699012  0.670103  0.000000  0.005574   \n2    0.185889  0.057895  0.635417  0.681941  0.639175  0.000000  0.003086   \n3    0.179104  0.059474  0.625000  0.538185  0.628866  0.000000  0.003056   \n4    0.172320  0.061053  0.572917  0.601078  0.577320  0.000000  0.002240   \n..        ...       ...       ...       ...       ...       ...       ...   \n339  0.267300  0.022632  0.041667  0.374663  0.783505  0.112936  0.014511   \n340  0.260516  0.023158  0.833333  0.316262  0.814433  0.127310  0.013753   \n341  0.227951  0.027368  0.833333  0.482480  0.814433  0.324435  0.000063   \n342  0.366350  0.023684  0.687500  0.495957  0.731959  0.369610  0.000505   \n343  0.360923  0.024211  0.750000  0.379155  0.752577  0.418891  0.002685   \n\n           14        15        16        17        18        19  \n0    0.028598  0.633333  0.616487  0.519523  0.358974  0.200920  \n1    0.003134  0.659259  0.641577  0.502169  0.339744  0.552834  \n2    0.002314  0.692593  0.673835  0.469631  0.269231  0.763411  \n3    0.022563  0.700000  0.681004  0.450108  0.250000  0.554998  \n4    0.002195  0.707407  0.688172  0.439262  0.230769  0.654322  \n..        ...       ...       ...       ...       ...       ...  \n339  0.012461  0.237037  0.222222  0.600868  0.500000  0.374290  \n340  0.012092  0.240741  0.225806  0.588937  0.480769  0.325180  \n341  0.010497  0.255556  0.243728  0.519523  0.410256  0.634602  \n342  0.001171  0.274074  0.261649  0.454447  0.346154  0.479199  \n343  0.011493  0.285185  0.275986  0.456616  0.333333  0.498023  \n\n[344 rows x 20 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.441860</td>\n      <td>0.375358</td>\n      <td>0.044286</td>\n      <td>0.000000</td>\n      <td>0.004170</td>\n      <td>0.649485</td>\n      <td>0.009290</td>\n      <td>0.232022</td>\n      <td>0.043684</td>\n      <td>0.031250</td>\n      <td>0.666667</td>\n      <td>0.649485</td>\n      <td>0.000000</td>\n      <td>0.004855</td>\n      <td>0.028598</td>\n      <td>0.633333</td>\n      <td>0.616487</td>\n      <td>0.519523</td>\n      <td>0.358974</td>\n      <td>0.200920</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.313953</td>\n      <td>0.388252</td>\n      <td>0.049286</td>\n      <td>0.000000</td>\n      <td>0.004575</td>\n      <td>0.670103</td>\n      <td>0.022436</td>\n      <td>0.211669</td>\n      <td>0.048947</td>\n      <td>0.666667</td>\n      <td>0.699012</td>\n      <td>0.670103</td>\n      <td>0.000000</td>\n      <td>0.005574</td>\n      <td>0.003134</td>\n      <td>0.659259</td>\n      <td>0.641577</td>\n      <td>0.502169</td>\n      <td>0.339744</td>\n      <td>0.552834</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.281395</td>\n      <td>0.409742</td>\n      <td>0.057143</td>\n      <td>0.001120</td>\n      <td>0.001503</td>\n      <td>0.639175</td>\n      <td>0.012872</td>\n      <td>0.185889</td>\n      <td>0.057895</td>\n      <td>0.635417</td>\n      <td>0.681941</td>\n      <td>0.639175</td>\n      <td>0.000000</td>\n      <td>0.003086</td>\n      <td>0.002314</td>\n      <td>0.692593</td>\n      <td>0.673835</td>\n      <td>0.469631</td>\n      <td>0.269231</td>\n      <td>0.763411</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.267442</td>\n      <td>0.421203</td>\n      <td>0.058571</td>\n      <td>0.000560</td>\n      <td>0.000625</td>\n      <td>0.628866</td>\n      <td>0.009185</td>\n      <td>0.179104</td>\n      <td>0.059474</td>\n      <td>0.625000</td>\n      <td>0.538185</td>\n      <td>0.628866</td>\n      <td>0.000000</td>\n      <td>0.003056</td>\n      <td>0.022563</td>\n      <td>0.700000</td>\n      <td>0.681004</td>\n      <td>0.450108</td>\n      <td>0.250000</td>\n      <td>0.554998</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.262791</td>\n      <td>0.421203</td>\n      <td>0.060000</td>\n      <td>0.001120</td>\n      <td>0.000992</td>\n      <td>0.639175</td>\n      <td>0.016020</td>\n      <td>0.172320</td>\n      <td>0.061053</td>\n      <td>0.572917</td>\n      <td>0.601078</td>\n      <td>0.577320</td>\n      <td>0.000000</td>\n      <td>0.002240</td>\n      <td>0.002195</td>\n      <td>0.707407</td>\n      <td>0.688172</td>\n      <td>0.439262</td>\n      <td>0.230769</td>\n      <td>0.654322</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>339</th>\n      <td>0.306977</td>\n      <td>0.498567</td>\n      <td>0.020714</td>\n      <td>0.144457</td>\n      <td>0.011548</td>\n      <td>0.783505</td>\n      <td>0.007213</td>\n      <td>0.267300</td>\n      <td>0.022632</td>\n      <td>0.041667</td>\n      <td>0.374663</td>\n      <td>0.783505</td>\n      <td>0.112936</td>\n      <td>0.014511</td>\n      <td>0.012461</td>\n      <td>0.237037</td>\n      <td>0.222222</td>\n      <td>0.600868</td>\n      <td>0.500000</td>\n      <td>0.374290</td>\n    </tr>\n    <tr>\n      <th>340</th>\n      <td>0.283721</td>\n      <td>0.522923</td>\n      <td>0.020714</td>\n      <td>0.143337</td>\n      <td>0.010733</td>\n      <td>0.814433</td>\n      <td>0.106538</td>\n      <td>0.260516</td>\n      <td>0.023158</td>\n      <td>0.833333</td>\n      <td>0.316262</td>\n      <td>0.814433</td>\n      <td>0.127310</td>\n      <td>0.013753</td>\n      <td>0.012092</td>\n      <td>0.240741</td>\n      <td>0.225806</td>\n      <td>0.588937</td>\n      <td>0.480769</td>\n      <td>0.325180</td>\n    </tr>\n    <tr>\n      <th>341</th>\n      <td>0.279070</td>\n      <td>0.752149</td>\n      <td>0.023571</td>\n      <td>0.125420</td>\n      <td>0.000093</td>\n      <td>0.814433</td>\n      <td>0.003695</td>\n      <td>0.227951</td>\n      <td>0.027368</td>\n      <td>0.833333</td>\n      <td>0.482480</td>\n      <td>0.814433</td>\n      <td>0.324435</td>\n      <td>0.000063</td>\n      <td>0.010497</td>\n      <td>0.255556</td>\n      <td>0.243728</td>\n      <td>0.519523</td>\n      <td>0.410256</td>\n      <td>0.634602</td>\n    </tr>\n    <tr>\n      <th>342</th>\n      <td>0.093023</td>\n      <td>0.839542</td>\n      <td>0.021429</td>\n      <td>0.259239</td>\n      <td>0.000045</td>\n      <td>0.731959</td>\n      <td>0.006867</td>\n      <td>0.366350</td>\n      <td>0.023684</td>\n      <td>0.687500</td>\n      <td>0.495957</td>\n      <td>0.731959</td>\n      <td>0.369610</td>\n      <td>0.000505</td>\n      <td>0.001171</td>\n      <td>0.274074</td>\n      <td>0.261649</td>\n      <td>0.454447</td>\n      <td>0.346154</td>\n      <td>0.479199</td>\n    </tr>\n    <tr>\n      <th>343</th>\n      <td>0.051163</td>\n      <td>0.904011</td>\n      <td>0.021429</td>\n      <td>0.198768</td>\n      <td>0.001208</td>\n      <td>0.752577</td>\n      <td>0.000000</td>\n      <td>0.360923</td>\n      <td>0.024211</td>\n      <td>0.750000</td>\n      <td>0.379155</td>\n      <td>0.752577</td>\n      <td>0.418891</td>\n      <td>0.002685</td>\n      <td>0.011493</td>\n      <td>0.285185</td>\n      <td>0.275986</td>\n      <td>0.456616</td>\n      <td>0.333333</td>\n      <td>0.498023</td>\n    </tr>\n  </tbody>\n</table>\n<p>344 rows × 20 columns</p>\n</div>"
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "important-grove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.37396122, 0.36980609, 0.37950139, ..., 0.09972299, 0.94875346,\n       0.91966759])"
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate training X and Y values\n",
    "train_X = training_data.iloc[:, 2:20].values\n",
    "train_Y = training_data.iloc[:, 1].values\n",
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "24b2aa58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[4.00000000e-02, 0.00000000e+00, 3.87755023e-03, ...,\n        5.08547009e-01, 3.51515152e-01, 7.68400194e-01],\n       [4.12500000e-02, 0.00000000e+00, 3.86149974e-03, ...,\n        5.02136752e-01, 3.45454545e-01, 5.92493555e-01],\n       [4.43750000e-02, 0.00000000e+00, 3.74293456e-04, ...,\n        4.85042735e-01, 3.21212121e-01, 6.30310819e-01],\n       ...,\n       [1.56250000e-02, 2.55491329e-01, 0.00000000e+00, ...,\n        4.56196581e-01, 3.51515152e-01, 4.56825484e-01],\n       [1.56250000e-02, 9.88439306e-02, 0.00000000e+00, ...,\n        4.56196581e-01, 3.39393939e-01, 8.31597725e-01],\n       [1.50000000e-02, 9.65317919e-02, 0.00000000e+00, ...,\n        4.63675214e-01, 3.39393939e-01, 3.18852773e-01]])"
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do not include the year column\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2cbccc29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.37535817, 0.38825215, 0.40974212, 0.42120344, 0.42120344,\n       0.        , 0.0243553 , 0.02005731, 0.15902579, 0.16475645,\n       0.1747851 , 0.17908309, 0.01862464, 0.51575931, 0.17191977,\n       0.17908309, 0.18051576, 0.19484241, 0.1747851 , 0.20057307,\n       0.20200573, 0.10315186, 0.10888252, 0.1747851 , 0.23065903,\n       0.21919771, 0.20916905, 0.21489971, 0.36103152, 0.10744986,\n       0.1260745 , 0.14040115, 0.26504298, 0.25787966, 0.2722063 ,\n       0.36962751, 0.40401146, 0.40544413, 0.40687679, 0.31232092,\n       0.33094556, 0.38968481, 0.46418338, 0.56160458, 0.93266476,\n       1.        , 0.02292264, 0.26074499, 0.2234957 , 0.22922636,\n       0.02148997, 0.23209169, 0.39255014, 0.0487106 , 0.52722063,\n       0.18338109, 0.52005731, 0.05157593, 0.00429799, 0.09598854,\n       0.11031519, 0.62464183, 0.64756447, 0.51719198, 0.52005731,\n       0.56303725, 0.11747851, 0.12893983, 0.13180516, 0.14040115,\n       0.01432665, 0.16332378, 0.20487106, 0.23782235, 0.35100287,\n       0.38681948, 0.13610315, 0.02148997, 0.13180516, 0.01862464,\n       0.00859599, 0.03868195, 0.30515759, 0.17191977, 0.19054441,\n       0.2234957 , 0.22492837, 0.28080229, 0.30372493, 0.04441261,\n       0.42550143, 0.48997135, 0.18051576, 0.26934097, 0.26790831,\n       0.26934097, 0.33810888, 0.35530086, 0.02578797, 0.27363897,\n       0.04011461, 0.31518625, 0.39398281, 0.04297994, 0.42550143,\n       0.41117479, 0.18194842, 0.17191977, 0.10028653, 0.10458453,\n       0.10744986, 0.11174785, 0.12034384, 0.36246418, 0.37535817,\n       0.04727794, 0.10744986, 0.01002865, 0.11747851, 0.11891117,\n       0.26647564, 0.27507163, 0.03295129, 0.03868195, 0.40544413,\n       0.04011461, 0.42406877, 0.32664756, 0.31518625, 0.33954155,\n       0.36103152, 0.02005731, 0.21489971, 0.23638968, 0.03581662,\n       0.25501433, 0.26934097, 0.26790831, 0.35530086, 0.09312321,\n       0.08595989, 0.09598854, 0.10028653, 0.0974212 , 0.08882521,\n       0.1017192 , 0.19340974, 0.19627507, 0.20200573, 0.16332378,\n       0.16475645, 0.17765043, 0.39398281, 0.58882521, 0.62464183,\n       0.03581662, 0.03724928, 0.30229226, 0.23209169, 0.30802292,\n       0.3252149 , 0.14040115, 0.13323782, 0.13896848, 0.01432665,\n       0.75358166, 0.37249284, 0.38825215, 0.0243553 , 0.03295129,\n       0.09169054, 0.1017192 , 0.13467049, 0.32091691, 0.36246418,\n       0.36962751, 0.37535817, 0.38252149, 0.09312321, 0.17908309,\n       0.18624642, 0.19197708, 0.01862464, 0.09455587, 0.12464183,\n       0.13180516, 0.17621777, 0.19770774, 0.38825215, 0.38968481,\n       0.39398281, 0.4025788 , 0.04011461, 0.08166189, 0.09455587,\n       0.20916905, 0.18051576, 0.17908309, 0.32091691, 0.39111748,\n       0.39111748, 0.16045845, 0.16618911, 0.18911175, 0.22063037,\n       0.02148997, 0.03438395, 0.4226361 , 0.33094556, 0.23782235,\n       0.25358166, 0.26934097, 0.31088825, 0.00716332, 0.23209169,\n       0.31805158, 0.34383954, 0.35959885, 0.53438395, 0.53581662,\n       0.55157593, 0.56590258, 0.23209169, 0.25358166, 0.25501433,\n       0.00143266, 0.26361032, 0.16905444, 0.17335244, 0.18051576,\n       0.40544413, 0.42120344, 0.4269341 , 0.45845272, 0.46704871,\n       0.21060172, 0.2234957 , 0.22636103, 0.22922636, 0.23495702,\n       0.18911175, 0.21633238, 0.31088825, 0.30945559, 0.30945559,\n       0.17765043, 0.20487106, 0.14040115, 0.19197708, 0.21776504,\n       0.21919771, 0.23495702, 0.25358166, 0.32664756, 0.33810888,\n       0.00286533, 0.46704871, 0.45988539, 0.0530086 , 0.03151862,\n       0.33237822, 0.34097421, 0.03581662, 0.4025788 , 0.42120344,\n       0.46848138, 0.18194842, 0.24498567, 0.24928367, 0.27363897,\n       0.32091691, 0.33667622, 0.35243553, 0.17908309, 0.18194842,\n       0.18767908, 0.26504298, 0.03295129, 0.30659026, 0.49570201,\n       0.64183381, 0.70916905, 0.08166189, 0.10315186, 0.10888252,\n       0.2034384 , 0.28080229, 0.25501433, 0.25358166, 0.03008596,\n       0.56160458, 0.81232092, 0.17191977, 0.18624642, 0.19054441,\n       0.19197708, 0.23638968, 0.247851  , 0.27507163, 0.02148997,\n       0.23065903, 0.23495702, 0.23925501, 0.26934097, 0.2765043 ,\n       0.23925501, 0.20200573, 0.02005731, 0.21489971, 0.21776504,\n       0.25501433, 0.02578797, 0.01575931, 0.01575931, 0.01432665,\n       0.16189112, 0.01862464, 0.2034384 , 0.32664756, 0.46418338,\n       0.05730659, 0.05730659, 0.63753582, 0.747851  , 0.03151862,\n       0.30229226, 0.38968481, 0.15759312, 0.17621777, 0.1747851 ,\n       0.1747851 , 0.26361032, 0.26217765, 0.26074499, 0.26217765,\n       0.27363897, 0.26074499, 0.20057307, 0.23495702, 0.49856734,\n       0.52292264, 0.752149  , 0.83954155, 0.90401146])"
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load testing values\n",
    "test_X = testing_data.iloc[:, 2:20].values\n",
    "test_Y = testing_data.iloc[:, 1].values\n",
    "test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e2ac0408",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[4.42857143e-02, 0.00000000e+00, 4.16977468e-03, ...,\n        5.19522777e-01, 3.58974359e-01, 2.00919979e-01],\n       [4.92857143e-02, 0.00000000e+00, 4.57525326e-03, ...,\n        5.02169197e-01, 3.39743590e-01, 5.52834106e-01],\n       [5.71428571e-02, 1.11982083e-03, 1.50328270e-03, ...,\n        4.69631236e-01, 2.69230769e-01, 7.63410942e-01],\n       ...,\n       [2.35714286e-02, 1.25419933e-01, 9.30427532e-05, ...,\n        5.19522777e-01, 4.10256410e-01, 6.34602020e-01],\n       [2.14285714e-02, 2.59238522e-01, 4.48885086e-05, ...,\n        4.54446855e-01, 3.46153846e-01, 4.79198899e-01],\n       [2.14285714e-02, 1.98768197e-01, 1.20786209e-03, ...,\n        4.56616052e-01, 3.33333333e-01, 4.98022847e-01]])"
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "0148558f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average baseline error:  0.19\n"
     ]
    }
   ],
   "source": [
    "#basline estimate\n",
    "#getting life_expectancy from the oroginal testing data\n",
    "baseline_preds = testing_data.iloc[:, 1:2].values\n",
    "# print(baseline_preds)\n",
    "# Baseline errors, and display average baseline error\n",
    "base_errors = abs(baseline_preds - test_Y)\n",
    "print('Average baseline error: ', round(np.mean(base_errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "cef65bf4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31774993 0.31824757 0.36222099 0.37005222 0.36667875 0.13515107\n",
      " 0.11851769 0.1197974  0.11261626 0.12697157 0.11269439 0.12531834\n",
      " 0.1795968  0.47728053 0.10796912 0.10805163 0.13967447 0.13623108\n",
      " 0.12902195 0.13867692 0.16199968 0.08062939 0.08222501 0.15533903\n",
      " 0.18959783 0.19224822 0.1797902  0.17640435 0.16090215 0.08215308\n",
      " 0.09466068 0.05677647 0.16132633 0.16173363 0.21298719 0.36473027\n",
      " 0.38979617 0.38606959 0.28471472 0.25080687 0.32407885 0.30771524\n",
      " 0.28862089 0.42569386 0.70478842 0.50215916 0.20912676 0.22490618\n",
      " 0.182392   0.15606387 0.18842567 0.14962017 0.3136485  0.39325502\n",
      " 0.41445544 0.20060686 0.4663537  0.49458128 0.48531158 0.07302541\n",
      " 0.08006812 0.50834751 0.43410309 0.30291153 0.43065545 0.47250534\n",
      " 0.08706757 0.14764448 0.13882095 0.12649975 0.10834507 0.0910194\n",
      " 0.13844711 0.17737699 0.28341728 0.34547174 0.12634978 0.13310661\n",
      " 0.12529654 0.08969669 0.09030357 0.22768426 0.14663699 0.14012791\n",
      " 0.15863918 0.19342422 0.21474244 0.21768749 0.20368654 0.34761412\n",
      " 0.31777177 0.40326222 0.14060153 0.19483731 0.22301744 0.22725151\n",
      " 0.3165827  0.31556289 0.15950337 0.18001771 0.11798802 0.20415838\n",
      " 0.36210598 0.37813103 0.40691759 0.3358685  0.13091301 0.16610753\n",
      " 0.07674604 0.08879022 0.06884688 0.06476355 0.06062167 0.27659896\n",
      " 0.30164225 0.35784437 0.07970023 0.09718119 0.08486879 0.09541024\n",
      " 0.21724643 0.21719758 0.20761979 0.20425195 0.36588638 0.29910423\n",
      " 0.29526032 0.19186433 0.24469343 0.23803593 0.25872647 0.20095447\n",
      " 0.19868063 0.21188916 0.14979225 0.2157497  0.21409697 0.22393826\n",
      " 0.21491092 0.07805802 0.03987588 0.06809798 0.08499925 0.07948779\n",
      " 0.05903465 0.08418485 0.1472079  0.15498488 0.1525622  0.14620861\n",
      " 0.14367679 0.16128512 0.26801182 0.4008522  0.59533048 0.05952456\n",
      " 0.06545669 0.12662731 0.10706595 0.17679557 0.15672688 0.11625972\n",
      " 0.11827935 0.12045951 0.11992576 0.68052845 0.37508087 0.43437846\n",
      " 0.21854847 0.26332124 0.06571869 0.06642938 0.09165639 0.21156758\n",
      " 0.2482232  0.25788748 0.28809002 0.30427512 0.68682282 0.16218664\n",
      " 0.13472709 0.16491702 0.18213752 0.09582404 0.09253319 0.09315988\n",
      " 0.13904811 0.14019753 0.36324404 0.35051455 0.36775014 0.37382463\n",
      " 0.3803222  0.08291517 0.08875906 0.15929041 0.14383525 0.1569974\n",
      " 0.18812607 0.24725913 0.26338928 0.11494726 0.11260673 0.12114764\n",
      " 0.10663468 0.13938064 0.13683261 0.21419762 0.40680275 0.20668376\n",
      " 0.22086003 0.22958535 0.22538989 0.06579041 0.18318237 0.34913347\n",
      " 0.34478975 0.33579072 0.36954211 0.40549376 0.44083561 0.40261761\n",
      " 0.18323769 0.21508861 0.19282521 0.20736196 0.24530536 0.11002437\n",
      " 0.16118924 0.15583008 0.31735986 0.25273407 0.27258252 0.31476844\n",
      " 0.36599287 0.16885815 0.18093785 0.17476191 0.19518248 0.17795743\n",
      " 0.12206858 0.16469008 0.28054574 0.24776185 0.28674474 0.09510228\n",
      " 0.1781943  0.09293569 0.14808107 0.19824036 0.19069184 0.18337846\n",
      " 0.22007006 0.24912742 0.26941444 0.38276822 0.37537544 0.35425923\n",
      " 0.31039517 0.20977145 0.20947165 0.28593435 0.33867726 0.3427116\n",
      " 0.3704333  0.36190775 0.1117983  0.15823367 0.20324904 0.2019269\n",
      " 0.1997897  0.29050551 0.29859464 0.16072897 0.16825418 0.19543898\n",
      " 0.25194527 0.16912319 0.15011441 0.40732135 0.64807445 0.66920152\n",
      " 0.06690374 0.08162594 0.0907499  0.19143032 0.18400913 0.15879996\n",
      " 0.16606456 0.16613805 0.45699551 0.48991598 0.14424444 0.18467624\n",
      " 0.14599048 0.18152693 0.22660681 0.22856863 0.23835845 0.19669844\n",
      " 0.20026922 0.23689246 0.24558392 0.25005444 0.23498965 0.20785377\n",
      " 0.16317663 0.12626818 0.13661068 0.15899642 0.18612052 0.17875217\n",
      " 0.05553223 0.06011025 0.06980167 0.12034907 0.13914671 0.12938537\n",
      " 0.28839493 0.41904337 0.30222423 0.38019426 0.41341457 0.204304\n",
      " 0.20835655 0.13598492 0.23221223 0.14080155 0.14926023 0.15366348\n",
      " 0.15398581 0.19366504 0.20941241 0.21946886 0.21397419 0.23416215\n",
      " 0.24617682 0.19372638 0.22763121 0.45071685 0.46747462 0.54888111\n",
      " 0.44579072 0.48338908]\n"
     ]
    }
   ],
   "source": [
    "#Training Model using RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 300, max_leaf_nodes = 300, bootstrap = True)\n",
    "# Fit the regressor with x and y data\n",
    "regressor.fit(train_X, train_Y)\n",
    "# Predict with testing X\n",
    "predict_Y = regressor.predict(test_X)\n",
    "print(predict_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "8954ac93",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.08 degrees.\n"
     ]
    }
   ],
   "source": [
    "# making prediction on the test set, we will get the prediction life expectancy\n",
    "predict_Y = regressor.predict(test_X)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predict_Y - test_Y)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "ba1c2a22",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.48 %.\n"
     ]
    }
   ],
   "source": [
    "#Determine Performance Metrics\n",
    "# Calculate mean absolute percentage error\n",
    "percentage_error = 100 * (errors / predict_Y)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(percentage_error)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "eced860a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:                   11 Importance: 0.49\n",
      "Variable:                   16 Importance: 0.09\n",
      "Variable:                    2 Importance: 0.04\n",
      "Variable:                    9 Importance: 0.04\n",
      "Variable:                   15 Importance: 0.04\n",
      "Variable:                    3 Importance: 0.03\n",
      "Variable:                    6 Importance: 0.03\n",
      "Variable:                   12 Importance: 0.03\n",
      "Variable:                   13 Importance: 0.03\n",
      "Variable:                   14 Importance: 0.03\n",
      "Variable:                   17 Importance: 0.03\n",
      "Variable:                    4 Importance: 0.02\n",
      "Variable:                    5 Importance: 0.02\n",
      "Variable:                    8 Importance: 0.02\n",
      "Variable:                   10 Importance: 0.02\n",
      "Variable:                   18 Importance: 0.02\n",
      "Variable:                    1 Importance: 0.01\n",
      "Variable:                    7 Importance: 0.01\n"
     ]
    }
   ],
   "source": [
    "#figuring out the usefulness of all the variable in the entire random forest\n",
    "# Get numerical feature importances\n",
    "testing_data_without_year = testing_data.drop(testing_data.columns[[0]],axis = 1)\n",
    "# print(testing_data_without_year)\n",
    "importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(testing_data_without_year, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0356118d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#picking up the most important variables to recalculate the importance\n",
    "updated_testing = testing_data.iloc[:, [2,6,9,11,12,13,14,15,13,16,17]]\n",
    "test_important = testing_data.iloc[:, [2,6,9,11,12,13,14,15,13,16,17]].values\n",
    "train_important = training_data.iloc[:, [2,6,9,11,12,13,14,15,13,16,17]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3cd56408",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:                   17 Importance: 0.49\n",
      "Variable:                    6 Importance: 0.04\n",
      "Variable:                   13 Importance: 0.04\n",
      "Variable:                    9 Importance: 0.03\n",
      "Variable:                   13 Importance: 0.03\n",
      "Variable:                   11 Importance: 0.02\n",
      "Variable:                   12 Importance: 0.02\n",
      "Variable:                   15 Importance: 0.02\n",
      "Variable:                   16 Importance: 0.02\n",
      "Variable:                    2 Importance: 0.01\n",
      "Variable:                   14 Importance: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "updated_importances = list(regressor.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(updated_testing, updated_importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "9e27e732",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.08141372280159816 degrees.\n",
      "Accuracy: -inf %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xu842\\AppData\\Local\\Temp\\ipykernel_64432\\2633863536.py:10: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  updated_error_percentage = np.mean(100 * (errors / test_Y))\n"
     ]
    }
   ],
   "source": [
    "# New random forest with only the two most important variables\n",
    "rf_most_important = RandomForestRegressor(n_estimators = 300, max_leaf_nodes = 300, bootstrap = True)\n",
    "# Train the random forest\n",
    "rf_most_important.fit(train_important, train_Y)\n",
    "# Make predictions and determine the error\n",
    "predictions = rf_most_important.predict(test_important)\n",
    "errors = abs(predictions - test_Y)\n",
    "# Display the performance metrics\n",
    "print('Mean Absolute Error:', np.mean(errors), 'degrees.')\n",
    "updated_error_percentage = np.mean(100 * (errors / test_Y))\n",
    "accuracy = 100 - updated_error_percentage\n",
    "print('Accuracy:', accuracy, '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a985c98a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of FixedLocator locations (18), usually from a call to set_ticks, does not match the number of ticklabels (20).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [291]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(x_value)\n\u001B[0;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mbar(x_value, importances, orientation \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mxticks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtesting_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrotation\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m6\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m plt\u001B[38;5;241m.\u001B[39mylabel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimportance\u001B[39m\u001B[38;5;124m'\u001B[39m);\n\u001B[0;32m     10\u001B[0m plt\u001B[38;5;241m.\u001B[39mxlable(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVariable\u001B[39m\u001B[38;5;124m'\u001B[39m);\n",
      "File \u001B[1;32mc:\\users\\xu842\\pycharmprojects\\leetcode2021python\\venv\\lib\\site-packages\\matplotlib\\pyplot.py:1816\u001B[0m, in \u001B[0;36mxticks\u001B[1;34m(ticks, labels, **kwargs)\u001B[0m\n\u001B[0;32m   1814\u001B[0m         l\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n\u001B[0;32m   1815\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1816\u001B[0m     labels \u001B[38;5;241m=\u001B[39m ax\u001B[38;5;241m.\u001B[39mset_xticklabels(labels, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1818\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m locs, labels\n",
      "File \u001B[1;32mc:\\users\\xu842\\pycharmprojects\\leetcode2021python\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py:75\u001B[0m, in \u001B[0;36m_axis_method_wrapper.__set_name__.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m get_method(\u001B[38;5;28mself\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\xu842\\pycharmprojects\\leetcode2021python\\venv\\lib\\site-packages\\matplotlib\\axis.py:1798\u001B[0m, in \u001B[0;36mAxis._set_ticklabels\u001B[1;34m(self, labels, fontdict, minor, **kwargs)\u001B[0m\n\u001B[0;32m   1796\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fontdict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1797\u001B[0m     kwargs\u001B[38;5;241m.\u001B[39mupdate(fontdict)\n\u001B[1;32m-> 1798\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_ticklabels(labels, minor\u001B[38;5;241m=\u001B[39mminor, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\xu842\\pycharmprojects\\leetcode2021python\\venv\\lib\\site-packages\\matplotlib\\axis.py:1720\u001B[0m, in \u001B[0;36mAxis.set_ticklabels\u001B[1;34m(self, ticklabels, minor, **kwargs)\u001B[0m\n\u001B[0;32m   1716\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(locator, mticker\u001B[38;5;241m.\u001B[39mFixedLocator):\n\u001B[0;32m   1717\u001B[0m     \u001B[38;5;66;03m# Passing [] as a list of ticklabels is often used as a way to\u001B[39;00m\n\u001B[0;32m   1718\u001B[0m     \u001B[38;5;66;03m# remove all tick labels, so only error for > 0 ticklabels\u001B[39;00m\n\u001B[0;32m   1719\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(locator\u001B[38;5;241m.\u001B[39mlocs) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(ticklabels) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(ticklabels) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 1720\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1721\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe number of FixedLocator locations\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1722\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(locator\u001B[38;5;241m.\u001B[39mlocs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m), usually from a call to\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1723\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m set_ticks, does not match\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1724\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m the number of ticklabels (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(ticklabels)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1725\u001B[0m     tickd \u001B[38;5;241m=\u001B[39m {loc: lab \u001B[38;5;28;01mfor\u001B[39;00m loc, lab \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(locator\u001B[38;5;241m.\u001B[39mlocs, ticklabels)}\n\u001B[0;32m   1726\u001B[0m     func \u001B[38;5;241m=\u001B[39m functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_with_dict, tickd)\n",
      "\u001B[1;31mValueError\u001B[0m: The number of FixedLocator locations (18), usually from a call to set_ticks, does not match the number of ticklabels (20)."
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_value = list(range(len(importances)))\n",
    "print(x_value)\n",
    "\n",
    "plt.bar(x_value, importances, orientation = 'vertical')\n",
    "plt.xticks(x_value, testing_data, rotation = 6)\n",
    "\n",
    "plt.ylabel('importance');\n",
    "plt.xlable('Variable');\n",
    "plt.title('Variable Importance');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "meaningful-reading",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE loss on testing set: 0.015083745129231064\n",
      "Random forest regressor score: 0.5012044611063322\n"
     ]
    }
   ],
   "source": [
    "errors = mean_squared_error(test_Y, predict_Y)\n",
    "print(\"MSE loss on testing set: \" + str(mean_squared_error(test_Y, predict_Y)))\n",
    "print(\"Random forest regressor score: \" + str(regressor.score(test_X, test_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f84d1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}